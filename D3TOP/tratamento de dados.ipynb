{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import re\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "tqdm.pandas()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Leitura de bases"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Os dados usados no desenvolvimento deste projeto surgiram a partir das bases presentes no link a seguir:\n",
    "\n",
    "https://www.kaggle.com/datasets/natlee/myanimelist-comment-dataset\n",
    "\n",
    "Osberve que a informação mais importante para nós são as que estão presentes na base `animeListGenres`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['id', 'workId', 'reviewId', 'workName', 'postTime', 'episodesSeen', 'author', 'peopleFoundUseful', 'overallRating', 'storyRating', 'animationRating', 'soundRating', 'characterRating', 'enjoymentRating', 'review']\n",
      "The number of column:  15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Now parsing... : 100%|██████████| 135206/135206 [00:14<00:00, 9347.88it/s] \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>workId</th>\n",
       "      <th>reviewId</th>\n",
       "      <th>workName</th>\n",
       "      <th>postTime</th>\n",
       "      <th>episodesSeen</th>\n",
       "      <th>author</th>\n",
       "      <th>peopleFoundUseful</th>\n",
       "      <th>overallRating</th>\n",
       "      <th>storyRating</th>\n",
       "      <th>animationRating</th>\n",
       "      <th>soundRating</th>\n",
       "      <th>characterRating</th>\n",
       "      <th>enjoymentRating</th>\n",
       "      <th>review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8121</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Cowboy_Bebop</td>\n",
       "      <td>2006-11-07 18:34:00</td>\n",
       "      <td>26</td>\n",
       "      <td>Xinil</td>\n",
       "      <td>162</td>\n",
       "      <td>10</td>\n",
       "      <td>8</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>Cowboy Bebop is an episodic series. By episodi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>63480</td>\n",
       "      <td>856</td>\n",
       "      <td>9</td>\n",
       "      <td>Utawarerumono</td>\n",
       "      <td>2006-11-08 00:24:00</td>\n",
       "      <td>26</td>\n",
       "      <td>Crystal</td>\n",
       "      <td>218</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>9</td>\n",
       "      <td>8</td>\n",
       "      <td>Utawarerumono manages to be one of those harem...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8452</td>\n",
       "      <td>263</td>\n",
       "      <td>10</td>\n",
       "      <td>Hajime_no_Ippo</td>\n",
       "      <td>2006-11-08 14:39:00</td>\n",
       "      <td>76</td>\n",
       "      <td>Xinil</td>\n",
       "      <td>827</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>First, let me say that I\\\\'m not a fan of boxi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>66544</td>\n",
       "      <td>129</td>\n",
       "      <td>11</td>\n",
       "      <td>Gensoumaden_Saiyuuki</td>\n",
       "      <td>2006-11-09 05:22:00</td>\n",
       "      <td>50</td>\n",
       "      <td>Chelle</td>\n",
       "      <td>69</td>\n",
       "      <td>9</td>\n",
       "      <td>8</td>\n",
       "      <td>6</td>\n",
       "      <td>7</td>\n",
       "      <td>10</td>\n",
       "      <td>9</td>\n",
       "      <td>Saiyuki is one of those animes that just grabs...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>55936</td>\n",
       "      <td>210</td>\n",
       "      <td>12</td>\n",
       "      <td>Ranma_½</td>\n",
       "      <td>2006-11-09 16:48:00</td>\n",
       "      <td>161</td>\n",
       "      <td>running_lemon</td>\n",
       "      <td>40</td>\n",
       "      <td>7</td>\n",
       "      <td>6</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>7</td>\n",
       "      <td>A comedy/romance based on the manga by Rumiko ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      id workId reviewId              workName             postTime  \\\n",
       "0   8121      1        1          Cowboy_Bebop  2006-11-07 18:34:00   \n",
       "1  63480    856        9         Utawarerumono  2006-11-08 00:24:00   \n",
       "2   8452    263       10        Hajime_no_Ippo  2006-11-08 14:39:00   \n",
       "3  66544    129       11  Gensoumaden_Saiyuuki  2006-11-09 05:22:00   \n",
       "4  55936    210       12               Ranma_½  2006-11-09 16:48:00   \n",
       "\n",
       "  episodesSeen         author peopleFoundUseful overallRating storyRating  \\\n",
       "0           26          Xinil               162            10           8   \n",
       "1           26        Crystal               218             8           8   \n",
       "2           76          Xinil               827            10          10   \n",
       "3           50         Chelle                69             9           8   \n",
       "4          161  running_lemon                40             7           6   \n",
       "\n",
       "  animationRating soundRating characterRating enjoymentRating  \\\n",
       "0              10          10              10              10   \n",
       "1               7           7               9               8   \n",
       "2               9           9              10              10   \n",
       "3               6           7              10               9   \n",
       "4               8           8               8               7   \n",
       "\n",
       "                                              review  \n",
       "0  Cowboy Bebop is an episodic series. By episodi...  \n",
       "1  Utawarerumono manages to be one of those harem...  \n",
       "2  First, let me say that I\\\\'m not a fan of boxi...  \n",
       "3  Saiyuki is one of those animes that just grabs...  \n",
       "4  A comedy/romance based on the manga by Rumiko ...  "
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nRowsRead = 'None' # specify 'None' if want to read whole file\n",
    "\n",
    "with open('animeReviewsOrderByTime.csv', 'r', encoding='utf-8') as f:\n",
    "    headers = f.readline().replace('\"','').replace('\\n','').split(',')\n",
    "    print(headers)\n",
    "    print('The number of column: ', len(headers))\n",
    "    dataFormat = dict()\n",
    "    for header in headers:\n",
    "        dataFormat[header] = list()\n",
    "\n",
    "    for idx, line in enumerate(tqdm(f.readlines(), desc='Now parsing... ')):\n",
    "        \n",
    "        if idx == 67:\n",
    "            yee = line\n",
    "        \n",
    "        if line != '':\n",
    "            line = line.replace('\\n','')\n",
    "            indices = [i for i, x in enumerate(line) if x == ',']\n",
    "            idxStart = 0\n",
    "            for i in range(len(headers)):\n",
    "                if i < len(headers) - 1:\n",
    "                    dataFormat[headers[i]].append(line[idxStart + 1:indices[i] - 1])\n",
    "                    idxStart = indices[i] + 1\n",
    "                elif i == len(headers) - 1:\n",
    "                    dataFormat[headers[i]].append(line[idxStart + 1:-1])\n",
    "                else:\n",
    "                    break\n",
    "        if nRowsRead is not None and nRowsRead == idx + 1:\n",
    "            print('We read only', nRowsRead, 'lines.')\n",
    "            break\n",
    "aval = pd.DataFrame(dataFormat)\n",
    "aval.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#listinha serve para avaliar quais ids tivemoserro ao coletar a sinopse\n",
    "listinha = []\n",
    "def get_anime_synopsis(anime_id):\n",
    "    time.sleep(1)\n",
    "    url = f'https://myanimelist.net/anime/{anime_id}'\n",
    "    response = requests.get(url)\n",
    "    \n",
    "    \n",
    "    if response.status_code != 200:\n",
    "        print(anime_id, \"Parou aqui\")\n",
    "        listinha.append(anime_id)\n",
    "        return \"-\" , \"-\"\n",
    "    \n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    synopsis_element = soup.find(\"p\", itemprop=\"description\")\n",
    "    synopsis = synopsis_element.text.strip() if synopsis_element else 'Synopsis not found.'\n",
    "    for image_tag in soup.find_all(\"img\",itemprop=\"image\"):\n",
    "        image_link = image_tag['data-src']\n",
    "\n",
    "    return synopsis , image_link"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como não temos a sinopse dos anims vamos coleta-las diretod a fonte, coletando de 100 em 100 e pausando por 10 min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carregando os dados\n",
    "animes = pd.read_excel(\"Nase_animes_tratados.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/8 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6546 Parou aqui\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▌       | 2/8 [14:21<36:06, 361.08s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An error occurred: local variable 'image_link' referenced before assignment. Ignoring and moving on to the next chunk.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 38%|███▊      | 3/8 [14:32<16:47, 201.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An error occurred: local variable 'image_link' referenced before assignment. Ignoring and moving on to the next chunk.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 4/8 [16:10<10:42, 160.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An error occurred: local variable 'image_link' referenced before assignment. Ignoring and moving on to the next chunk.\n",
      "39186 Parou aqui\n",
      "39143 Parou aqui\n",
      "37644 Parou aqui\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 62%|██████▎   | 5/8 [17:43<06:47, 135.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An error occurred: local variable 'image_link' referenced before assignment. Ignoring and moving on to the next chunk.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|███████▌  | 6/8 [17:49<03:03, 91.90s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An error occurred: local variable 'image_link' referenced before assignment. Ignoring and moving on to the next chunk.\n",
      "39413 Parou aqui\n",
      "39405 Parou aqui\n",
      "39406 Parou aqui\n",
      "39536 Parou aqui\n",
      "39502 Parou aqui\n",
      "39501 Parou aqui\n",
      "39540 Parou aqui\n",
      "39315 Parou aqui\n",
      "39564 Parou aqui\n",
      "39517 Parou aqui\n",
      "39310 Parou aqui\n",
      "39342 Parou aqui\n",
      "39344 Parou aqui\n",
      "39343 Parou aqui\n",
      "39384 Parou aqui\n",
      "39550 Parou aqui\n",
      "39322 Parou aqui\n",
      "39369 Parou aqui\n",
      "39370 Parou aqui\n",
      "39407 Parou aqui\n",
      "39307 Parou aqui\n",
      "39431 Parou aqui\n",
      "39621 Parou aqui\n",
      "39330 Parou aqui\n",
      "39376 Parou aqui\n",
      "39720 Parou aqui\n",
      "39375 Parou aqui\n",
      "39541 Parou aqui\n",
      "39546 Parou aqui\n",
      "39374 Parou aqui\n",
      "39403 Parou aqui\n",
      "39371 Parou aqui\n",
      "39372 Parou aqui\n",
      "39306 Parou aqui\n",
      "39566 Parou aqui\n",
      "39488 Parou aqui\n",
      "39333 Parou aqui\n",
      "39360 Parou aqui\n",
      "39457 Parou aqui\n",
      "39544 Parou aqui\n",
      "39507 Parou aqui\n",
      "39568 Parou aqui\n",
      "39513 Parou aqui\n",
      "39358 Parou aqui\n",
      "39361 Parou aqui\n",
      "39359 Parou aqui\n",
      "39335 Parou aqui\n",
      "39503 Parou aqui\n",
      "39453 Parou aqui\n",
      "39352 Parou aqui\n",
      "39581 Parou aqui\n",
      "39451 Parou aqui\n",
      "39349 Parou aqui\n",
      "39428 Parou aqui\n",
      "39309 Parou aqui\n",
      "39668 Parou aqui\n",
      "39454 Parou aqui\n",
      "39455 Parou aqui\n",
      "39430 Parou aqui\n",
      "39450 Parou aqui\n",
      "39331 Parou aqui\n",
      "39610 Parou aqui\n",
      "39590 Parou aqui\n",
      "39555 Parou aqui\n",
      "39565 Parou aqui\n",
      "39523 Parou aqui\n",
      "39351 Parou aqui\n",
      "39519 Parou aqui\n",
      "39545 Parou aqui\n",
      "39533 Parou aqui\n",
      "39463 Parou aqui\n",
      "39576 Parou aqui\n",
      "39586 Parou aqui\n",
      "39663 Parou aqui\n",
      "39570 Parou aqui\n",
      "39468 Parou aqui\n",
      "39584 Parou aqui\n",
      "39534 Parou aqui\n",
      "39326 Parou aqui\n",
      "39567 Parou aqui\n",
      "39582 Parou aqui\n",
      "39388 Parou aqui\n",
      "39531 Parou aqui\n",
      "39574 Parou aqui\n",
      "39487 Parou aqui\n",
      "39585 Parou aqui\n",
      "39382 Parou aqui\n",
      "39535 Parou aqui\n",
      "39701 Parou aqui\n",
      "39539 Parou aqui\n",
      "39583 Parou aqui\n",
      "39609 Parou aqui\n",
      "39474 Parou aqui\n",
      "39506 Parou aqui\n",
      "39491 Parou aqui\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8/8 [40:32<00:00, 304.07s/it]\n"
     ]
    }
   ],
   "source": [
    "#Coletando sinopse dos animes\n",
    "for i in tqdm(len(animes)):\n",
    "    try:\n",
    "        start = i * 100\n",
    "        end = (i + 1) * 100 - 1\n",
    "        result=pd.DataFrame()\n",
    "        result[['synopsis', 'IMG']] = animes.loc[start:end, 'workId'].apply(get_anime_synopsis).apply(pd.Series)\n",
    "        animes.loc[start:end, ['synopsis', 'IMG']] = result\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}. Ignoring and moving on to the next chunk.\")\n",
    "        continue\n",
    "    animes.to_excel(\"Nase_animes_tratados.xlsx\", index = False)\n",
    "    time.sleep(600)  # Sleep for 600 seconds (10 minutes) after each chunk"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Tratamento dos dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "animes = animes[(animes[\"IMG\"] !=\"-\") & (animes[\"synopsis\"] !=\"-\")]\n",
    "# Removendo duplicatas\n",
    "animes.drop_duplicates(inplace=True)\n",
    "aval.drop_duplicates(inplace=True)\n",
    "\n",
    "# Tratando valores faltantes\n",
    "animes.fillna(\"\", inplace=True)\n",
    "aval.fillna(\"\", inplace=True)\n",
    "\n",
    "# Instanciando o stemmer and the lemmatizer\n",
    "stemmer = PorterStemmer()\n",
    "lemmatizer = WordNetLemmatizer()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package rslp to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package rslp is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Baixe as stopwords em português (se ainda não tiver baixado)\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Carregue as stopwords em português\n",
    "remover_palavras = stopwords.words('english')#stopwords.words('portuguese')\n",
    "\n",
    "from nltk.stem import RSLPStemmer\n",
    "import spacy\n",
    "\n",
    "# Baixe os recursos necessários (se ainda não tiver baixado)\n",
    "nltk.download('rslp')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# Crie uma instância do stemmer RSLP\n",
    "stemmer = RSLPStemmer()\n",
    "\n",
    "# Crie uma instância do lematizador para o português\n",
    "nlp = spacy.load('en_core_web_sm')#spacy.load('pt_core_news_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to remove HTML tags\n",
    "def remove_html(text):\n",
    "    soup = BeautifulSoup(text, \"html.parser\")\n",
    "    return soup.get_text()\n",
    "\n",
    "# Function to remove URLs\n",
    "def remove_urls(text):\n",
    "    url_pattern = re.compile(r'https?://\\S+|www\\.\\S+')\n",
    "    return url_pattern.sub(r'', text)\n",
    "\n",
    "# Function to remove emojis\n",
    "def remove_emojis(text):\n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                           u\"\\U00002702-\\U000027B0\"\n",
    "                           u\"\\U000024C2-\\U0001F251\"\n",
    "                           \"]+\", flags=re.UNICODE)\n",
    "    return emoji_pattern.sub(r'', text)\n",
    "\n",
    "# Function to preprocess the text\n",
    "def preprocess(text):\n",
    "    try:\n",
    "        # Removing HTML tags\n",
    "        text = remove_html(text)\n",
    "    except:\n",
    "        pass\n",
    "        # Removing URLs\n",
    "    try:\n",
    "        text = remove_urls(text)\n",
    "    except:\n",
    "        pass\n",
    "        # Removing emojis\n",
    "    try:\n",
    "        text = remove_emojis(text)\n",
    "    except:\n",
    "        pass\n",
    "        # Converting to lowercase\n",
    "    try:\n",
    "        text = text.lower()\n",
    "    except:\n",
    "        pass\n",
    "        # Removing special characters\n",
    "    try:\n",
    "        text = re.sub(r'\\W', ' ', text)\n",
    "    except:\n",
    "        pass\n",
    "        # Removing extra spaces\n",
    "    try:\n",
    "        text = re.sub(r'\\s+', ' ', text, flags=re.I)\n",
    "    except:\n",
    "        pass\n",
    "        # Removing stopwords, applying lemmatization and then stemming\n",
    "    try:\n",
    "        #text = ' '.join([stemmer.stem(lemmatizer.lemmatize(word)) for word in text.split() if word not in remover_palavras])\n",
    "        text = ' '.join([stemmer.stem(token.lemma_) for token in nlp(text) if token.lemma_ not in remover_palavras])\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14728/14728 [01:28<00:00, 167.11it/s] \n",
      " 32%|███▏      | 43044/135206 [23:56<51:15, 29.97it/s]  \n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "string index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[31], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39m# Applying the preprocess function to the synopsis and user comments\u001b[39;00m\n\u001b[0;32m      2\u001b[0m animes[\u001b[39m'\u001b[39m\u001b[39msynopsis tratada\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m animes[\u001b[39m'\u001b[39m\u001b[39msynopsis\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mprogress_apply(preprocess)\n\u001b[1;32m----> 3\u001b[0m aval[\u001b[39m'\u001b[39m\u001b[39mreview tratada\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m aval[\u001b[39m'\u001b[39;49m\u001b[39mreview\u001b[39;49m\u001b[39m'\u001b[39;49m]\u001b[39m.\u001b[39;49mprogress_apply(preprocess)\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\lib\\site-packages\\tqdm\\std.py:814\u001b[0m, in \u001b[0;36mtqdm.pandas.<locals>.inner_generator.<locals>.inner\u001b[1;34m(df, func, *args, **kwargs)\u001b[0m\n\u001b[0;32m    811\u001b[0m \u001b[39m# Apply the provided function (in **kwargs)\u001b[39;00m\n\u001b[0;32m    812\u001b[0m \u001b[39m# on the df using our wrapper (which provides bar updating)\u001b[39;00m\n\u001b[0;32m    813\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 814\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mgetattr\u001b[39m(df, df_function)(wrapper, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    815\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m    816\u001b[0m     t\u001b[39m.\u001b[39mclose()\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\lib\\site-packages\\pandas\\core\\series.py:4771\u001b[0m, in \u001b[0;36mSeries.apply\u001b[1;34m(self, func, convert_dtype, args, **kwargs)\u001b[0m\n\u001b[0;32m   4661\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mapply\u001b[39m(\n\u001b[0;32m   4662\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m   4663\u001b[0m     func: AggFuncType,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   4666\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[0;32m   4667\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m DataFrame \u001b[39m|\u001b[39m Series:\n\u001b[0;32m   4668\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m   4669\u001b[0m \u001b[39m    Invoke function on values of Series.\u001b[39;00m\n\u001b[0;32m   4670\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   4769\u001b[0m \u001b[39m    dtype: float64\u001b[39;00m\n\u001b[0;32m   4770\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 4771\u001b[0m     \u001b[39mreturn\u001b[39;00m SeriesApply(\u001b[39mself\u001b[39;49m, func, convert_dtype, args, kwargs)\u001b[39m.\u001b[39;49mapply()\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\lib\\site-packages\\pandas\\core\\apply.py:1123\u001b[0m, in \u001b[0;36mSeriesApply.apply\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1120\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mapply_str()\n\u001b[0;32m   1122\u001b[0m \u001b[39m# self.f is Callable\u001b[39;00m\n\u001b[1;32m-> 1123\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mapply_standard()\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\lib\\site-packages\\pandas\\core\\apply.py:1174\u001b[0m, in \u001b[0;36mSeriesApply.apply_standard\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1172\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   1173\u001b[0m         values \u001b[39m=\u001b[39m obj\u001b[39m.\u001b[39mastype(\u001b[39mobject\u001b[39m)\u001b[39m.\u001b[39m_values\n\u001b[1;32m-> 1174\u001b[0m         mapped \u001b[39m=\u001b[39m lib\u001b[39m.\u001b[39;49mmap_infer(\n\u001b[0;32m   1175\u001b[0m             values,\n\u001b[0;32m   1176\u001b[0m             f,\n\u001b[0;32m   1177\u001b[0m             convert\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconvert_dtype,\n\u001b[0;32m   1178\u001b[0m         )\n\u001b[0;32m   1180\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(mapped) \u001b[39mand\u001b[39;00m \u001b[39misinstance\u001b[39m(mapped[\u001b[39m0\u001b[39m], ABCSeries):\n\u001b[0;32m   1181\u001b[0m     \u001b[39m# GH#43986 Need to do list(mapped) in order to get treated as nested\u001b[39;00m\n\u001b[0;32m   1182\u001b[0m     \u001b[39m#  See also GH#25959 regarding EA support\u001b[39;00m\n\u001b[0;32m   1183\u001b[0m     \u001b[39mreturn\u001b[39;00m obj\u001b[39m.\u001b[39m_constructor_expanddim(\u001b[39mlist\u001b[39m(mapped), index\u001b[39m=\u001b[39mobj\u001b[39m.\u001b[39mindex)\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\lib\\site-packages\\pandas\\_libs\\lib.pyx:2924\u001b[0m, in \u001b[0;36mpandas._libs.lib.map_infer\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\lib\\site-packages\\tqdm\\std.py:809\u001b[0m, in \u001b[0;36mtqdm.pandas.<locals>.inner_generator.<locals>.inner.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    803\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mwrapper\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m    804\u001b[0m     \u001b[39m# update tbar correctly\u001b[39;00m\n\u001b[0;32m    805\u001b[0m     \u001b[39m# it seems `pandas apply` calls `func` twice\u001b[39;00m\n\u001b[0;32m    806\u001b[0m     \u001b[39m# on the first column/row to decide whether it can\u001b[39;00m\n\u001b[0;32m    807\u001b[0m     \u001b[39m# take a fast or slow code path; so stop when t.total==t.n\u001b[39;00m\n\u001b[0;32m    808\u001b[0m     t\u001b[39m.\u001b[39mupdate(n\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m t\u001b[39m.\u001b[39mtotal \u001b[39mor\u001b[39;00m t\u001b[39m.\u001b[39mn \u001b[39m<\u001b[39m t\u001b[39m.\u001b[39mtotal \u001b[39melse\u001b[39;00m \u001b[39m0\u001b[39m)\n\u001b[1;32m--> 809\u001b[0m     \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "Cell \u001b[1;32mIn[30], line 39\u001b[0m, in \u001b[0;36mpreprocess\u001b[1;34m(text)\u001b[0m\n\u001b[0;32m     36\u001b[0m text \u001b[39m=\u001b[39m re\u001b[39m.\u001b[39msub(\u001b[39mr\u001b[39m\u001b[39m'\u001b[39m\u001b[39m\\\u001b[39m\u001b[39ms+\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39m \u001b[39m\u001b[39m'\u001b[39m, text, flags\u001b[39m=\u001b[39mre\u001b[39m.\u001b[39mI)\n\u001b[0;32m     37\u001b[0m \u001b[39m# Removing stopwords, applying lemmatization and then stemming\u001b[39;00m\n\u001b[0;32m     38\u001b[0m \u001b[39m#text = ' '.join([stemmer.stem(lemmatizer.lemmatize(word)) for word in text.split() if word not in remover_palavras])\u001b[39;00m\n\u001b[1;32m---> 39\u001b[0m text \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39m \u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mjoin([stemmer\u001b[39m.\u001b[39mstem(token\u001b[39m.\u001b[39mlemma_) \u001b[39mfor\u001b[39;00m token \u001b[39min\u001b[39;00m nlp(text) \u001b[39mif\u001b[39;00m token\u001b[39m.\u001b[39mlemma_ \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m remover_palavras])\n\u001b[0;32m     41\u001b[0m \u001b[39mreturn\u001b[39;00m text\n",
      "Cell \u001b[1;32mIn[30], line 39\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     36\u001b[0m text \u001b[39m=\u001b[39m re\u001b[39m.\u001b[39msub(\u001b[39mr\u001b[39m\u001b[39m'\u001b[39m\u001b[39m\\\u001b[39m\u001b[39ms+\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39m \u001b[39m\u001b[39m'\u001b[39m, text, flags\u001b[39m=\u001b[39mre\u001b[39m.\u001b[39mI)\n\u001b[0;32m     37\u001b[0m \u001b[39m# Removing stopwords, applying lemmatization and then stemming\u001b[39;00m\n\u001b[0;32m     38\u001b[0m \u001b[39m#text = ' '.join([stemmer.stem(lemmatizer.lemmatize(word)) for word in text.split() if word not in remover_palavras])\u001b[39;00m\n\u001b[1;32m---> 39\u001b[0m text \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39m \u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mjoin([stemmer\u001b[39m.\u001b[39;49mstem(token\u001b[39m.\u001b[39;49mlemma_) \u001b[39mfor\u001b[39;00m token \u001b[39min\u001b[39;00m nlp(text) \u001b[39mif\u001b[39;00m token\u001b[39m.\u001b[39mlemma_ \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m remover_palavras])\n\u001b[0;32m     41\u001b[0m \u001b[39mreturn\u001b[39;00m text\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\lib\\site-packages\\nltk\\stem\\rslp.py:101\u001b[0m, in \u001b[0;36mRSLPStemmer.stem\u001b[1;34m(self, word)\u001b[0m\n\u001b[0;32m     98\u001b[0m word \u001b[39m=\u001b[39m word\u001b[39m.\u001b[39mlower()\n\u001b[0;32m    100\u001b[0m \u001b[39m# the word ends in 's'? apply rule for plural reduction\u001b[39;00m\n\u001b[1;32m--> 101\u001b[0m \u001b[39mif\u001b[39;00m word[\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m] \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39ms\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m    102\u001b[0m     word \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mapply_rule(word, \u001b[39m0\u001b[39m)\n\u001b[0;32m    104\u001b[0m \u001b[39m# the word ends in 'a'? apply rule for feminine reduction\u001b[39;00m\n",
      "\u001b[1;31mIndexError\u001b[0m: string index out of range"
     ]
    }
   ],
   "source": [
    "# Applying the preprocess function to the synopsis and user comments\n",
    "animes['synopsis tratada'] = animes['synopsis'].progress_apply(preprocess)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 135206/135206 [2:08:57<00:00, 17.47it/s]  \n"
     ]
    }
   ],
   "source": [
    "aval['review tratada'] = aval['review'].progress_apply(preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14728/14728 [01:43<00:00, 142.31it/s] \n"
     ]
    }
   ],
   "source": [
    "animes['synopsis tratada final'] = animes['synopsis tratada'].progress_apply(preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "animes.reset_index(inplace= True, drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\\N                                                         6265\n",
       "Cyborg 009                                                    4\n",
       "Outlaw Star                                                   3\n",
       "The Heroic Legend of Arslan                                   3\n",
       "JoJo's Bizarre Adventure                                      3\n",
       "                                                           ... \n",
       "Astro Boy (1980)                                              1\n",
       "Armored Trooper Votoms: The Last Red Shoulder                 1\n",
       "Shuffle!                                                      1\n",
       "Spaceketeers                                                  1\n",
       "UzaMaid! My Maid Is Still Seriously Way Too Annoying...       1\n",
       "Name: engName, Length: 5162, dtype: int64"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Existem alguns nomes em branco\n",
    "animes[\"engName\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#atualizando nome dos animes\n",
    "def atualizar_nome(row):\n",
    "    if row[\"engName\"] != \"\\\\N\":\n",
    "        return row[\"engName\"]\n",
    "    elif row[\"synonymsName\"] != \"\\\\N\":\n",
    "        return row[\"synonymsName\"]\n",
    "    else:\n",
    "        return row[\"jpName\"]\n",
    "animes[\"name\"] = animes.apply(lambda row: atualizar_nome(row), axis = 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Minna no Uta                                                                          67\n",
       "\\N                                                                                    13\n",
       "Irodorimidori                                                                          7\n",
       "Wakate Animator Ikusei Project, 2018 Young Animator Training Project, Anime Tamago     4\n",
       "Spooky Kitaro                                                                          4\n",
       "                                                                                      ..\n",
       "BROTHERS CONFLICT OVA                                                                  1\n",
       "Arjuna                                                                                 1\n",
       "Children Record                                                                        1\n",
       "CLAMP IN WONDERLAND                                                                    1\n",
       "UzaMaid! My Maid Is Still Seriously Way Too Annoying...                                1\n",
       "Name: name, Length: 11261, dtype: int64"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Note que reduzimos os sem nome para 13, vamos mantelos\n",
    "animes[\"name\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "animes = animes[-animes[\"synopsis tratada final\"].isna()].reset_index(drop=True)\n",
    "animes.to_excel(\"base_tratada_final_animes.xlsx\",index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "aval.to_csv(\"reviews_tratada.csv\", index = False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Similadirade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "aval = pd.read_csv(\"reviews_tratada.csv\")\n",
    "animes = pd.read_excel(\"base_tratada_final_animes.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "\n",
    "# Vectorize the preprocessed anime descriptions\n",
    "vectorizer = TfidfVectorizer()\n",
    "vectorized_descriptions = vectorizer.fit_transform(animes['synopsis tratada final'])\n",
    "\n",
    "# Calculate similarity scores\n",
    "similarity_matrix = cosine_similarity(vectorized_descriptions)\n",
    "\n",
    "# Function to retrieve top similar animes for a given anime\n",
    "def get_top_similar_animes(anime_index, similarity_matrix, top_k=5):\n",
    "    anime_scores = similarity_matrix[anime_index]  # Get similarity scores for the given anime\n",
    "    top_indices = anime_scores.argsort()[-top_k-1:-1][::-1]  # Get top similar anime indices (excluding the given anime itself)\n",
    "    return animes.iloc[top_indices]['workId'].values.tolist()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"vector\", similarity_matrix)   "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
