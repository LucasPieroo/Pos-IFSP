{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import re\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "tqdm.pandas()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Leitura de bases"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Os dados usados no desenvolvimento deste projeto surgiram a partir das bases presentes no link a seguir:\n",
    "\n",
    "https://www.kaggle.com/datasets/natlee/myanimelist-comment-dataset\n",
    "\n",
    "Osberve que a informação mais importante para nós são as que estão presentes na base `animeListGenres`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nRowsRead = 'None' # specify 'None' if want to read whole file\n",
    "\n",
    "with open('animeReviewsOrderByTime.csv', 'r', encoding='utf-8') as f:\n",
    "    headers = f.readline().replace('\"','').replace('\\n','').split(',')\n",
    "    print(headers)\n",
    "    print('The number of column: ', len(headers))\n",
    "    dataFormat = dict()\n",
    "    for header in headers:\n",
    "        dataFormat[header] = list()\n",
    "\n",
    "    for idx, line in enumerate(tqdm(f.readlines(), desc='Now parsing... ')):\n",
    "        \n",
    "        if idx == 67:\n",
    "            yee = line\n",
    "        \n",
    "        if line != '':\n",
    "            line = line.replace('\\n','')\n",
    "            indices = [i for i, x in enumerate(line) if x == ',']\n",
    "            idxStart = 0\n",
    "            for i in range(len(headers)):\n",
    "                if i < len(headers) - 1:\n",
    "                    dataFormat[headers[i]].append(line[idxStart + 1:indices[i] - 1])\n",
    "                    idxStart = indices[i] + 1\n",
    "                elif i == len(headers) - 1:\n",
    "                    dataFormat[headers[i]].append(line[idxStart + 1:-1])\n",
    "                else:\n",
    "                    break\n",
    "        if nRowsRead is not None and nRowsRead == idx + 1:\n",
    "            print('We read only', nRowsRead, 'lines.')\n",
    "            break\n",
    "aval = pd.DataFrame(dataFormat)\n",
    "aval.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#listinha serve para avaliar quais ids tivemoserro ao coletar a sinopse\n",
    "listinha = []\n",
    "def get_anime_synopsis(anime_id):\n",
    "    time.sleep(1)\n",
    "    url = f'https://myanimelist.net/anime/{anime_id}'\n",
    "    response = requests.get(url)\n",
    "    \n",
    "    \n",
    "    if response.status_code != 200:\n",
    "        print(anime_id, \"Parou aqui\")\n",
    "        listinha.append(anime_id)\n",
    "        return \"-\" , \"-\"\n",
    "    \n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    synopsis_element = soup.find(\"p\", itemprop=\"description\")\n",
    "    synopsis = synopsis_element.text.strip() if synopsis_element else 'Synopsis not found.'\n",
    "    for image_tag in soup.find_all(\"img\",itemprop=\"image\"):\n",
    "        image_link = image_tag['data-src']\n",
    "\n",
    "    return synopsis , image_link"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como não temos a sinopse dos anims vamos coleta-las diretod a fonte, coletando de 100 em 100 e pausando por 10 min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carregando os dados\n",
    "animes = pd.read_excel(\"Base_animes_tratados.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Coletando sinopse dos animes\n",
    "for i in tqdm(len(animes)):\n",
    "    try:\n",
    "        start = i * 100\n",
    "        end = (i + 1) * 100 - 1\n",
    "        result=pd.DataFrame()\n",
    "        result[['synopsis', 'IMG']] = animes.loc[start:end, 'workId'].apply(get_anime_synopsis).apply(pd.Series)\n",
    "        animes.loc[start:end, ['synopsis', 'IMG']] = result\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}. Ignoring and moving on to the next chunk.\")\n",
    "        continue\n",
    "    animes.to_excel(\"Nase_animes_tratados.xlsx\", index = False)\n",
    "    time.sleep(600)  # Sleep for 600 seconds (10 minutes) after each chunk"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Tratamento dos dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "animes = animes[(animes[\"IMG\"] !=\"-\") & (animes[\"synopsis\"] !=\"-\")]\n",
    "# Removendo duplicatas\n",
    "animes.drop_duplicates(inplace=True)\n",
    "aval.drop_duplicates(inplace=True)\n",
    "\n",
    "# Tratando valores faltantes\n",
    "animes.fillna(\"\", inplace=True)\n",
    "aval.fillna(\"\", inplace=True)\n",
    "\n",
    "# Instanciando o stemmer and the lemmatizer\n",
    "stemmer = PorterStemmer()\n",
    "lemmatizer = WordNetLemmatizer()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "animes.drop_duplicates(inplace=True)\n",
    "aval.drop_duplicates(inplace=True)\n",
    "\n",
    "# Tratando valores faltantes\n",
    "animes.fillna(\"\", inplace=True)\n",
    "aval.fillna(\"\", inplace=True)\n",
    "\n",
    "# Instanciando o stemmer and the lemmatizer\n",
    "stemmer = PorterStemmer()\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Baixe as stopwords em português (se ainda não tiver baixado)\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Carregue as stopwords em português\n",
    "remover_palavras = stopwords.words('english')#stopwords.words('portuguese')\n",
    "\n",
    "from nltk.stem import RSLPStemmer\n",
    "import spacy\n",
    "\n",
    "# Baixe os recursos necessários (se ainda não tiver baixado)\n",
    "nltk.download('rslp')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# Crie uma instância do stemmer RSLP\n",
    "stemmer = RSLPStemmer()\n",
    "\n",
    "# Crie uma instância do lematizador para o português\n",
    "nlp = spacy.load('en_core_web_sm')#spacy.load('pt_core_news_sm')\n",
    "# Function to remove HTML tags\n",
    "def remove_html(text):\n",
    "    soup = BeautifulSoup(text, \"html.parser\")\n",
    "    return soup.get_text()\n",
    "\n",
    "# Function to remove URLs\n",
    "def remove_urls(text):\n",
    "    url_pattern = re.compile(r'https?://\\S+|www\\.\\S+')\n",
    "    return url_pattern.sub(r'', text)\n",
    "\n",
    "# Function to remove emojis\n",
    "def remove_emojis(text):\n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                           u\"\\U00002702-\\U000027B0\"\n",
    "                           u\"\\U000024C2-\\U0001F251\"\n",
    "                           \"]+\", flags=re.UNICODE)\n",
    "    return emoji_pattern.sub(r'', text)\n",
    "\n",
    "# Function to preprocess the text\n",
    "def preprocess(text):\n",
    "    try:\n",
    "        # Removing HTML tags\n",
    "        text = remove_html(text)\n",
    "    except:\n",
    "        pass\n",
    "        # Removing URLs\n",
    "    try:\n",
    "        text = remove_urls(text)\n",
    "    except:\n",
    "        pass\n",
    "        # Removing emojis\n",
    "    try:\n",
    "        text = remove_emojis(text)\n",
    "    except:\n",
    "        pass\n",
    "        # Converting to lowercase\n",
    "    try:\n",
    "        text = text.lower()\n",
    "    except:\n",
    "        pass\n",
    "        # Removing special characters\n",
    "    try:\n",
    "        text = re.sub(r'\\W', ' ', text)\n",
    "    except:\n",
    "        pass\n",
    "        # Removing extra spaces\n",
    "    try:\n",
    "        text = re.sub(r'\\s+', ' ', text, flags=re.I)\n",
    "    except:\n",
    "        pass\n",
    "        # Removing stopwords, applying lemmatization and then stemming\n",
    "    try:\n",
    "        #text = ' '.join([stemmer.stem(lemmatizer.lemmatize(word)) for word in text.split() if word not in remover_palavras])\n",
    "        text = ' '.join([stemmer.stem(token.lemma_) for token in nlp(text) if token.lemma_ not in remover_palavras])\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to remove HTML tags\n",
    "def remove_html(text):\n",
    "    soup = BeautifulSoup(text, \"html.parser\")\n",
    "    return soup.get_text()\n",
    "\n",
    "# Function to remove URLs\n",
    "def remove_urls(text):\n",
    "    url_pattern = re.compile(r'https?://\\S+|www\\.\\S+')\n",
    "    return url_pattern.sub(r'', text)\n",
    "\n",
    "# Function to remove emojis\n",
    "def remove_emojis(text):\n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                           u\"\\U00002702-\\U000027B0\"\n",
    "                           u\"\\U000024C2-\\U0001F251\"\n",
    "                           \"]+\", flags=re.UNICODE)\n",
    "    return emoji_pattern.sub(r'', text)\n",
    "\n",
    "# Function to preprocess the text\n",
    "def preprocess(text):\n",
    "    try:\n",
    "        # Removing HTML tags\n",
    "        text = remove_html(text)\n",
    "    except:\n",
    "        pass\n",
    "        # Removing URLs\n",
    "    try:\n",
    "        text = remove_urls(text)\n",
    "    except:\n",
    "        pass\n",
    "        # Removing emojis\n",
    "    try:\n",
    "        text = remove_emojis(text)\n",
    "    except:\n",
    "        pass\n",
    "        # Converting to lowercase\n",
    "    try:\n",
    "        text = text.lower()\n",
    "    except:\n",
    "        pass\n",
    "        # Removing special characters\n",
    "    try:\n",
    "        text = re.sub(r'\\W', ' ', text)\n",
    "    except:\n",
    "        pass\n",
    "        # Removing extra spaces\n",
    "    try:\n",
    "        text = re.sub(r'\\s+', ' ', text, flags=re.I)\n",
    "    except:\n",
    "        pass\n",
    "        # Removing stopwords, applying lemmatization and then stemming\n",
    "    try:\n",
    "        #text = ' '.join([stemmer.stem(lemmatizer.lemmatize(word)) for word in text.split() if word not in remover_palavras])\n",
    "        text = ' '.join([stemmer.stem(token.lemma_) for token in nlp(text) if token.lemma_ not in remover_palavras])\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Applying the preprocess function to the synopsis and user comments\n",
    "animes['synopsis tratada'] = animes['synopsis'].progress_apply(preprocess)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aval['review tratada'] = aval['review'].progress_apply(preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "animes['synopsis tratada final'] = animes['synopsis tratada'].progress_apply(preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "animes.reset_index(inplace= True, drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Existem alguns nomes em branco\n",
    "animes[\"engName\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#atualizando nome dos animes\n",
    "def atualizar_nome(row):\n",
    "    if row[\"engName\"] != \"\\\\N\":\n",
    "        return row[\"engName\"]\n",
    "    elif row[\"synonymsName\"] != \"\\\\N\":\n",
    "        return row[\"synonymsName\"]\n",
    "    else:\n",
    "        return row[\"jpName\"]\n",
    "animes[\"name\"] = animes.apply(lambda row: atualizar_nome(row), axis = 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Note que reduzimos os sem nome para 13, vamos mantelos\n",
    "animes[\"name\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "animes = animes[-animes[\"synopsis tratada final\"].isna()].reset_index(drop=True)\n",
    "animes.to_excel(\"base_tratada_final_animes.xlsx\",index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aval.to_csv(\"reviews_tratada.csv\", index = False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Similadirade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aval = pd.read_csv(\"reviews_tratada.csv\")\n",
    "animes = pd.read_excel(\"base_tratada_final_animes.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "\n",
    "# Vectorize the preprocessed anime descriptions\n",
    "vectorizer = TfidfVectorizer()\n",
    "vectorized_descriptions = vectorizer.fit_transform(animes['synopsis tratada final'])\n",
    "\n",
    "# Calculate similarity scores\n",
    "similarity_matrix = cosine_similarity(vectorized_descriptions)\n",
    "\n",
    "# Function to retrieve top similar animes for a given anime\n",
    "def get_top_similar_animes(anime_index, similarity_matrix, top_k=5):\n",
    "    anime_scores = similarity_matrix[anime_index]  # Get similarity scores for the given anime\n",
    "    top_indices = anime_scores.argsort()[-top_k-1:-1][::-1]  # Get top similar anime indices (excluding the given anime itself)\n",
    "    return animes.iloc[top_indices]['workId'].values.tolist()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"vector\", similarity_matrix)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "animes[animes[\"name\"] == \"Wolf Children\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "similar_animes = get_top_similar_animes(34, similarity_matrix, top_k=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "genero = animes.loc[34, \"genres\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "genero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_auxiliar = animes[animes[\"workId\"].isin(similar_animes)]\n",
    "base_auxiliar[\"ordem\"] = pd.Categorical(base_auxiliar[\"workId\"], categories=similar_animes, ordered=True)\n",
    "base_auxiliar = base_auxiliar.sort_values(\"ordem\").reset_index()\n",
    "base_auxiliar.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate Fuzzy Ratio for each row\n",
    "base_auxiliar['fuzzy_ratio'] = base_auxiliar['genres'].apply(lambda x: fuzz.ratio(x, genero))\n",
    "\n",
    "\n",
    "\n",
    "def classifica(row):\n",
    "    if row['fuzzy_ratio'] > 0.8:\n",
    "        return 4\n",
    "    elif row['fuzzy_ratio'] > 0.6:\n",
    "        return 3\n",
    "    elif row['fuzzy_ratio'] > 0.4:\n",
    "        return 2\n",
    "    elif row['fuzzy_ratio'] > 0.2:\n",
    "        return 1\n",
    "    elif row['fuzzy_ratio'] > 0:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_auxiliar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_auxiliar['fuzzy_category'] = base_auxiliar.apply(lambda row: classifica(row) ,axis = 1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_auxiliar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort by Fuzzy Ratio categories, then by original order\n",
    "base_auxiliar.sort_values('fuzzy_category', ascending=False, kind='mergesort', inplace=True)\n",
    "\n",
    "# You can drop the helper columns if not needed\n",
    "base_auxiliar.drop(['fuzzy_ratio', 'fuzzy_category'], axis=1, inplace=True)\n",
    "base_auxiliar.reset_index(inplace = True , drop = True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
